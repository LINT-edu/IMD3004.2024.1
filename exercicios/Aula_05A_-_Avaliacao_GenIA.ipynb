{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOEx8XMm3kdj"
      },
      "source": [
        "# Material correspondente à Aula 05 - Avaliação de modelos generativos com enfase em GANS\n",
        "\n",
        "### Inspirado em: *DeepLearning.AI*, *Kaggle* e *Machine Learning Mastery*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBKTusutTZDI"
      },
      "source": [
        "## Installing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiP-2-TBTjU5",
        "outputId": "69c6004e-fbe1-48cd-aadb-19c058350b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.1->torchvision) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "\n",
        "!pip install torchvision\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzhtBNrNTkKv"
      },
      "source": [
        "## Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_V_s9JvTXvt",
        "outputId": "24506efa-f2db-4900-fe39-d773fef95cbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f516a87f0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torch import nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA\n",
        "from torchvision.models import inception_v3\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nEZ_hxwx_RY"
      },
      "source": [
        "### Criando a classe do gerador\n",
        "\n",
        "> **A classe Generator implementa um gerador para um Deep Convolutional GAN (DCGAN). Ela usa camadas convolucionais transpostas para gerar imagens a partir de vetores de ruído.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ATrk2w3swvD4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Classe Gerador\n",
        "    Argumentos:\n",
        "        z_dim: dimensão do vetor de ruído, um escalar.\n",
        "        im_chan: número de canais nas imagens geradas, ajustado ao conjunto de dados utilizado, um escalar.\n",
        "                 (Por exemplo, CelebA utiliza RGB, então o padrão é 3).\n",
        "        hidden_dim: dimensão interna das camadas, um escalar.\n",
        "    '''\n",
        "    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        # Construção da rede neural\n",
        "        self.gen = nn.Sequential(\n",
        "            self._criar_bloco_gerador(z_dim, hidden_dim * 8),\n",
        "            self._criar_bloco_gerador(hidden_dim * 8, hidden_dim * 4),\n",
        "            self._criar_bloco_gerador(hidden_dim * 4, hidden_dim * 2),\n",
        "            self._criar_bloco_gerador(hidden_dim * 2, hidden_dim),\n",
        "            self._criar_bloco_gerador(hidden_dim, im_chan, kernel_size=4, camada_final=True),\n",
        "        )\n",
        "\n",
        "    def _criar_bloco_gerador(self, canais_entrada, canais_saida, kernel_size=3, stride=2, camada_final=False):\n",
        "        '''\n",
        "        Cria um bloco do gerador para o DCGAN, contendo:\n",
        "        - Convolução transposta\n",
        "        - BatchNorm (exceto na camada final)\n",
        "        - Função de ativação\n",
        "        Argumentos:\n",
        "            canais_entrada: número de canais na representação de entrada.\n",
        "            canais_saida: número de canais na representação de saída.\n",
        "            kernel_size: tamanho do filtro convolucional (kernel_size, kernel_size).\n",
        "            stride: passo da convolução.\n",
        "            camada_final: booleano, True se for a camada final; afeta a ativação e o BatchNorm.\n",
        "        '''\n",
        "        if not camada_final:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(canais_entrada, canais_saida, kernel_size, stride),\n",
        "                nn.BatchNorm2d(canais_saida),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(canais_entrada, canais_saida, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, ruido):\n",
        "        '''\n",
        "        Passagem direta do gerador. Dado um tensor de ruído, retorna imagens geradas.\n",
        "        Argumentos:\n",
        "            ruido: tensor de ruído com dimensões (n_amostras, z_dim).\n",
        "        '''\n",
        "        x = ruido.view(len(ruido), self.z_dim, 1, 1)  # Redimensiona o ruído para a forma esperada\n",
        "        return self.gen(x)\n",
        "\n",
        "def gerar_ruido(n_amostras, z_dim, device='cpu'):\n",
        "    '''\n",
        "    Cria vetores de ruído aleatórios com dimensões (n_amostras, z_dim).\n",
        "    Argumentos:\n",
        "        n_amostras: número de vetores de ruído a serem gerados.\n",
        "        z_dim: dimensão de cada vetor de ruído.\n",
        "        device: dispositivo onde os tensores serão alocados ('cpu' ou 'cuda').\n",
        "    Retorna:\n",
        "        Tensor de ruído aleatório.\n",
        "    '''\n",
        "    return torch.randn(n_amostras, z_dim, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "VtAi38ywe3Tn"
      },
      "outputs": [],
      "source": [
        "# Dimensão do vetor latente (z_dim) usado como entrada para o modelo gerador\n",
        "z_dim = 64\n",
        "\n",
        "# Dimensão de saída da imagem (em pixels)\n",
        "image_size = 299\n",
        "\n",
        "# Define o dispositivo padrão como CPU\n",
        "device = 'cpu'\n",
        "\n",
        "# Verifica se há uma GPU disponível e, caso positivo, muda o dispositivo para CUDA (GPU)\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "# Define uma sequência de transformações para preprocessamento das imagens\n",
        "transform = transforms.Compose([\n",
        "    # Redimensiona a imagem para o tamanho especificado (299x299)\n",
        "    transforms.Resize(image_size),\n",
        "\n",
        "    # Realiza um recorte central na imagem para garantir o tamanho correto\n",
        "    transforms.CenterCrop(image_size),\n",
        "\n",
        "    # Converte a imagem para um tensor do PyTorch, normalizando automaticamente os valores para [0, 1]\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Normaliza os valores dos pixels para o intervalo [-1, 1]\n",
        "    # (0.5, 0.5, 0.5) é a média para cada canal (R, G, B)\n",
        "    # (0.5, 0.5, 0.5) é o desvio padrão para cada canal (R, G, B)\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8LwxK1GLTAY"
      },
      "source": [
        "### Carregando GAN pré-treinada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OIC9Fu8ANh_"
      },
      "source": [
        "> **O CelebA (CelebFaces Attributes Dataset) é um conjunto de dados amplamente utilizado em pesquisas de visão computacional, especialmente em tarefas relacionadas ao reconhecimento facial, edição de imagens e geração de imagens (como GANs). Ele contém mais de 200.000 imagens de rostos de celebridades, acompanhadas de 40 atributos faciais anotados (por exemplo, óculos, barba, sorriso) e localizações das principais características faciais. Para acessá-lo clique [aqui](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "NVZ88I021pvK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Carrega os dados de um arquivo .npz que contem features extraídas de imagens\n",
        "data = torch.Tensor(np.load('/content/drive/MyDrive/models/fid_images_tensor.npz', allow_pickle=True)['arr_0'])\n",
        "\n",
        "# Cria um TensorDataset dataset utilizando o PyTorch, onde os dados são usados tanto como entradas quanto como rótulos\n",
        "# TensorDataset associa duas ou mais fontes de dados (neste caso, data e data) para serem usados como pares (x, y) em treinamento ou avaliação.\n",
        "dataset = torch.utils.data.TensorDataset(data, data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qLZCT30EWwrw"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "\n",
        "  dataset = CelebA(\n",
        "    root=\"/content/drive/MyDrive/models/celeba\",  # Caminho onde as imagens estão extraídas\n",
        "    split=\"all\",  # Pode ser 'train', 'valid' ou 'test'\n",
        "    transform=transform,  # Transformações a serem aplicadas\n",
        "    download=False  # Não tenta baixar, pois o dataset já está disponível localmente\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "94pMnlv2xFFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfaac22-f376-4d71-f9b5-bcdde8c5cb52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-519d232e4809>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(\"/content/drive/MyDrive/models/pretrained_celeba.pth\", map_location=torch.device(device))[\"gen\"]\n"
          ]
        }
      ],
      "source": [
        "# Inicializa o gerador com a dimensão do vetor latente (z_dim) e move para o dispositivo especificado (CPU ou GPU)\n",
        "gen = Generator(z_dim).to(device)\n",
        "\n",
        "# Carrega o modelo pré-treinados Celeba\n",
        "gen.load_state_dict(\n",
        "    torch.load(\"/content/drive/MyDrive/models/pretrained_celeba.pth\", map_location=torch.device(device))[\"gen\"]\n",
        ")\n",
        "\n",
        "# Define o modelo gerador no modo de avaliação (evaluation mode)\n",
        "gen = gen.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CwgOOlFWP0j"
      },
      "source": [
        "## Carregando classificador pré-treinado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW4oAT7PbV4K"
      },
      "source": [
        "### Inception-v3 Network\n",
        "\n",
        "> **Inception-V3 é uma rede neural treinada no [ImageNet](http://www.image-net.org/) para classificar objetos. Você deve se lembrar das palestras que o ImageNet tem mais de 1 milhão de imagens para treinar. Como resultado, o Inception-V3 faz um bom trabalho detectando recursos e classificando imagens. Aqui, você carregará o Inception-V3 como inception_model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "u5YdjtVKbVdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea379ae-e535-4580-eb53-52756b9277fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-1e91d999f33f>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  inception_model.load_state_dict(torch.load(\"/content/drive/MyDrive/models/inception_v3_google-1a9a5a14.pth\"))\n"
          ]
        }
      ],
      "source": [
        "# Importa o modelo Inception v3 do PyTorch\n",
        "inception_model = inception_v3(pretrained = False)\n",
        "\n",
        "# Carrega os pesos pré-treinados para o modelo Inception v3\n",
        "inception_model.load_state_dict(torch.load(\"/content/drive/MyDrive/models/inception_v3_google-1a9a5a14.pth\"))\n",
        "\n",
        "# Move o modelo para o dispositivo especificado (CPU ou GPU)\n",
        "inception_model.to(device)\n",
        "\n",
        "# Define o modelo em modo de avaliação (evaluation mode)\n",
        "# Isso desativa o comportamento de dropout e batch normalization no modo de treinamento,\n",
        "# garantindo que o modelo opere de forma determinística durante a inferência.\n",
        "inception_model = inception_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C84O-k8Rz2cW"
      },
      "source": [
        "#### Removendo a última camada do Inception-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EwqrOscHbbZ0"
      },
      "outputs": [],
      "source": [
        "inception_model.fc = torch.nn.Identity()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCPBuiRjbntN"
      },
      "source": [
        "# 1. Por que estamos removendo a ultima camada do Inception-v3?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6jLhvNabsCr"
      },
      "source": [
        "> Resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqe1m6v9ywRC"
      },
      "source": [
        "### Implementando Frechet Incerption Distance (FID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3JW10aH_z82V"
      },
      "outputs": [],
      "source": [
        "def matrix_sqrt(x):\n",
        "    '''\n",
        "    Função que recebe uma matriz e retorna a raiz quadrada dessa matriz.\n",
        "    Para uma matriz de entrada A, a matriz de saída B será tal que B @ B é igual à matriz A.\n",
        "    Parâmetros:\n",
        "        x: uma matriz (Tensor do PyTorch)\n",
        "    Retorna:\n",
        "        A raiz quadrada da matriz x como um Tensor do PyTorch\n",
        "    '''\n",
        "    # Converter o tensor para numpy, garantindo que ele esteja na CPU e sem gradiente\n",
        "    y = x.cpu().detach().numpy()\n",
        "\n",
        "    # Calcular a raiz quadrada da matriz usando scipy.linalg.sqrtm\n",
        "    y = scipy.linalg.sqrtm(y)\n",
        "\n",
        "    # Retornar o resultado como um Tensor do PyTorch no mesmo dispositivo do tensor original\n",
        "    return torch.tensor(y.real, device=x.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "nZAKkfUf0I4T"
      },
      "outputs": [],
      "source": [
        "def frechet_distance(mu_x, mu_y, sigma_x, sigma_y):\n",
        "    '''\n",
        "    Função que calcula a distância de Fréchet entre distribuições Gaussianas multivariadas,\n",
        "    parametrizadas por suas médias e matrizes de covariância.\n",
        "\n",
        "    Parâmetros:\n",
        "        mu_x: média da primeira Gaussiana, (n_features)\n",
        "        mu_y: média da segunda Gaussiana, (n_features)\n",
        "        sigma_x: matriz de covariância da primeira Gaussiana, (n_features, n_features)\n",
        "        sigma_y: matriz de covariância da segunda Gaussiana, (n_features, n_features)\n",
        "\n",
        "    Retorna:\n",
        "        Distância de Fréchet (float).\n",
        "    '''\n",
        "    # Diferença entre as médias\n",
        "    mean_diff = mu_x - mu_y\n",
        "\n",
        "    # Cálculo da distância de Fréchet\n",
        "    distance = (mean_diff.dot(mean_diff) +\n",
        "                torch.trace(sigma_x) +\n",
        "                torch.trace(sigma_y) -\n",
        "                2 * torch.trace(matrix_sqrt(sigma_x @ sigma_y)))\n",
        "\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz7OnnVJ-lzo"
      },
      "source": [
        "#### Pre-processamento das imagens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5SvLN6dl0UH5"
      },
      "outputs": [],
      "source": [
        "def preprocess(img):\n",
        "    '''\n",
        "    Função para preprocessar uma imagem, redimensionando-a para 299x299\n",
        "    usando interpolação bilinear.\n",
        "\n",
        "    Parâmetros:\n",
        "        img: tensor representando a imagem de entrada.\n",
        "\n",
        "    Retorna:\n",
        "        A imagem redimensionada como um tensor.\n",
        "    '''\n",
        "    # Redimensionar a imagem para 299x299 usando interpolação bilinear\n",
        "    img = torch.nn.functional.interpolate(img, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSTMiATL_NkB"
      },
      "source": [
        "#### Cálculo de covariância"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mg3h7_sn0fUh"
      },
      "outputs": [],
      "source": [
        "def get_covariance(features):\n",
        "    '''\n",
        "    Função para calcular a matriz de covariância de um conjunto de características.\n",
        "\n",
        "    Parâmetros:\n",
        "        features: tensor do PyTorch contendo as características (amostras x características).\n",
        "\n",
        "    Retorna:\n",
        "        A matriz de covariância como um tensor do PyTorch.\n",
        "    '''\n",
        "    # Garantir que os dados sejam convertidos para um array numpy sem gradiente\n",
        "    features_np = features.detach().numpy()\n",
        "\n",
        "    # Calcular a matriz de covariância usando numpy, com as colunas como variáveis\n",
        "    covariance_matrix = np.cov(features_np, rowvar=False)\n",
        "\n",
        "    # Converter a matriz numpy para um tensor do PyTorch\n",
        "    return torch.tensor(covariance_matrix, dtype=features.dtype, device=features.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAK6QBeSK4xK"
      },
      "source": [
        "#### Extraindo features em imagens reais e geradas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60t0JhQw0klC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "eec633b0b05c4c0cb516209a5a4979e0",
            "583bc123216940b8a479679e3046388a",
            "157b0f206f734c139848184dd10e8967",
            "a6d3ef6bdc994da5beffbd5dc67df0d3",
            "f570e5a2c9f94c2e87efc890b6be72d4",
            "3f1c295ffc164fd38b23ce9b633587de",
            "4b0f25f23c064f8b88e7e69056cd5dba",
            "75518350a1bb4407b0b91fa4ed34e3ba",
            "d6e5799348ad4082986baa7a0b385ac6",
            "7bfab507f0464015aabbf110698bbe16",
            "028b4699b2904f1aba6f67ba19807c44"
          ]
        },
        "outputId": "db0eb91c-5ea8-43ec-a177-e906d262ced6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/128 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eec633b0b05c4c0cb516209a5a4979e0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "fake_features_list = []  # Lista para armazenar as características das imagens geradas\n",
        "real_features_list = []  # Lista para armazenar as características das imagens reais\n",
        "\n",
        "gen.eval()  # Coloca o gerador em modo de avaliação (inference mode)\n",
        "n_samples = 512  # Número total de amostras a serem processadas\n",
        "batch_size = 4  # Número de amostras por iteração (tamanho do lote)\n",
        "\n",
        "# Configura o DataLoader para iterar sobre o dataset real\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True  # Embaralha os dados para evitar ordens repetitivas\n",
        ")\n",
        "\n",
        "cur_samples = 0  # Contador de amostras processadas\n",
        "with torch.no_grad():  # Desativa o cálculo de gradientes para economizar memória\n",
        "    try:\n",
        "        for real_example, _ in tqdm(dataloader, total=n_samples // batch_size):  # Processa por lote\n",
        "            # Obtém as amostras reais\n",
        "            real_samples = real_example\n",
        "            # Extrai as características das imagens reais usando o modelo Inception\n",
        "            real_features = inception_model(real_samples.to(device)).detach().to('cpu')  # Move as características para a CPU\n",
        "            real_features_list.append(real_features)  # Adiciona à lista de características reais\n",
        "\n",
        "            # Gera amostras falsas usando ruído como entrada\n",
        "            fake_samples = get_noise(len(real_example), z_dim).to(device)  # Gera vetores de ruído\n",
        "            fake_samples = preprocess(gen(fake_samples))  # Gera imagens a partir do gerador e aplica preprocessamento\n",
        "            # Extrai as características das imagens geradas usando o modelo Inception\n",
        "            fake_features = inception_model(fake_samples.to(device)).detach().to('cpu')  # Move as características para a CPU\n",
        "            fake_features_list.append(fake_features)  # Adiciona à lista de características geradas\n",
        "\n",
        "            cur_samples += len(real_samples)  # Atualiza o contador de amostras processadas\n",
        "            if cur_samples >= n_samples:  # Interrompe o loop se o número total de amostras for atingido\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no loop: {e}\")  # Captura possíveis erros durante o processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0kmWTakZZr0"
      },
      "source": [
        "# 2. Explique qual a função da célula acima. Apresente o raciocínio lógico do código e que os produtos serão gerados a partir dele."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzT2cg0ea37w"
      },
      "source": [
        "> Resposta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combina todas as características (features) extraídas de imagens geradas e reais em tensores únicos\n",
        "fake_features_all = torch.cat(fake_features_list)\n",
        "real_features_all = torch.cat(real_features_list)"
      ],
      "metadata": {
        "id": "DLKuBsiut5jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MDltsV564SYy",
        "outputId": "bad64fef-492c-4f8a-b93f-3cd1a08622a6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'fake_features_all' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-76307e560e37>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calcula as estatisticas descritivas entre verdadeiros e falsos (gerados)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmu_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_features_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmu_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_features_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msigma_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_features_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fake_features_all' is not defined"
          ]
        }
      ],
      "source": [
        "# Calcula as estatisticas descritivas entre verdadeiros e falsos (gerados)\n",
        "mu_fake = fake_features_all.mean(0)\n",
        "mu_real = real_features_all.mean(0)\n",
        "\n",
        "sigma_fake = get_covariance(fake_features_all)\n",
        "sigma_real = get_covariance(real_features_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1U4JKG84_78"
      },
      "outputs": [],
      "source": [
        "# Seleciona embeddings e o número de amostras\n",
        "indices = [2, 4, 5]\n",
        "n_samples = 5000\n",
        "\n",
        "# Função auxiliar para criar amostras e dataframe\n",
        "def create_samples(mu, sigma, indices, label):\n",
        "    dist = MultivariateNormal(mu[indices], sigma[indices][:, indices])\n",
        "    samples = dist.sample((n_samples,)).numpy()\n",
        "    df = pd.DataFrame(samples, columns=indices)\n",
        "    df[\"is_real\"] = label\n",
        "    return df\n",
        "\n",
        "# Gerar amostras falsas e reais\n",
        "df_fake = create_samples(mu_fake, sigma_fake, indices, \"no\")\n",
        "df_real = create_samples(mu_real, sigma_real, indices, \"yes\")\n",
        "\n",
        "# Combinar e plotar\n",
        "df = pd.concat([df_fake, df_real])\n",
        "sns.pairplot(df, plot_kws={'alpha': 0.1}, hue='is_real')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJbEK2aIMB60"
      },
      "source": [
        "> **As distribuições permitem avaliar o quão bem as amostras geradas (falsas) replicam as características estatísticas das amostras reais, como médias, variâncias e correlações, além de identificar discrepâncias ou similaridades entre os dois conjuntos em um espaço multivariado definido pelos índices selecionados.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBSs122ubFoH"
      },
      "source": [
        "# 3. Qual sua interpretação a partir das tramas anteriores?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CItNMvTJbPgA"
      },
      "source": [
        "> Resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEianAz1bROK"
      },
      "source": [
        "### Resultado do FID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s0YVikf5BSL"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsowxZNWZ2aI"
      },
      "source": [
        "# 4. O que se pode reduzir a partir dessa distância do FID?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agy7Iv9Ka0_1"
      },
      "source": [
        "Resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alE8gQeyauOW"
      },
      "source": [
        "### Inspecionando visualmento o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuzrWxsxKInz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Função para plotar um lote de imagens\n",
        "def plot_images(images, title=\"Imagens\"):\n",
        "    \"\"\"\n",
        "    Plota um conjunto de imagens em um grid.\n",
        "    Parâmetros:\n",
        "        images: Tensor contendo as imagens no formato (batch_size, C, H, W).\n",
        "        title: Título do gráfico.\n",
        "    \"\"\"\n",
        "    # Normaliza as imagens para o intervalo [0, 1] para exibição\n",
        "    grid = torchvision.utils.make_grid(images, nrow=4, normalize=True, value_range=(-1, 1))\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(grid.permute(1, 2, 0))  # Ajusta as dimensões para exibição (H, W, C)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualizar imagens reais\n",
        "real_images, _ = next(iter(dataloader))  # Obtém o primeiro lote do dataloader\n",
        "plot_images(real_images, title=\"Imagens Reais do Dataset\")\n",
        "\n",
        "# Visualizar imagens geradas\n",
        "gen.eval()  # Garante que o gerador está no modo de avaliação\n",
        "with torch.no_grad():  # Desativa o cálculo de gradientes para eficiência\n",
        "    fake_noise = get_noise(batch_size, z_dim, device=device)  # Gera vetores de ruído\n",
        "    fake_images = gen(fake_noise).detach().cpu()  # Gera imagens e move para CPU\n",
        "    plot_images(fake_images, title=\"Imagens Geradas pelo Gerador\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpmTlQUANnEn"
      },
      "source": [
        "# 5. Com base no observado que outras melhorias poderiam ser empregadas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBFyF817a7Z-"
      },
      "source": [
        "> Resposta\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eec633b0b05c4c0cb516209a5a4979e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_583bc123216940b8a479679e3046388a",
              "IPY_MODEL_157b0f206f734c139848184dd10e8967",
              "IPY_MODEL_a6d3ef6bdc994da5beffbd5dc67df0d3"
            ],
            "layout": "IPY_MODEL_f570e5a2c9f94c2e87efc890b6be72d4"
          }
        },
        "583bc123216940b8a479679e3046388a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f1c295ffc164fd38b23ce9b633587de",
            "placeholder": "​",
            "style": "IPY_MODEL_4b0f25f23c064f8b88e7e69056cd5dba",
            "value": " 73%"
          }
        },
        "157b0f206f734c139848184dd10e8967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75518350a1bb4407b0b91fa4ed34e3ba",
            "max": 128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6e5799348ad4082986baa7a0b385ac6",
            "value": 94
          }
        },
        "a6d3ef6bdc994da5beffbd5dc67df0d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bfab507f0464015aabbf110698bbe16",
            "placeholder": "​",
            "style": "IPY_MODEL_028b4699b2904f1aba6f67ba19807c44",
            "value": " 94/128 [04:51&lt;01:36,  2.82s/it]"
          }
        },
        "f570e5a2c9f94c2e87efc890b6be72d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f1c295ffc164fd38b23ce9b633587de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b0f25f23c064f8b88e7e69056cd5dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75518350a1bb4407b0b91fa4ed34e3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e5799348ad4082986baa7a0b385ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7bfab507f0464015aabbf110698bbe16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "028b4699b2904f1aba6f67ba19807c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}